import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from io import StringIO

from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò –ö–û–ù–°–¢–ê–ù–¢–´ ---
RANDOM_STATE = 42
MAX_ORIGINAL_GRADE = 10.0
MAX_TARGET_GRADE = 100.0

# --- 1. –§–£–ù–ö–¶–ò–ò –û–ë–†–ê–ë–û–¢–ö–ò –î–ê–ù–ù–´–• –ò –ú–û–î–ï–õ–ò–†–û–í–ê–ù–ò–Ø ---

@st.cache_data
def load_and_preprocess_data(uploaded_file):
    """–ó–∞–≥—Ä—É–∑–∫–∞, –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö."""
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
    else:
        # –î–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –¥–µ–ø–ª–æ—è –∏–ª–∏ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω
        st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª 'student_prediction.csv'.")
        return None, None, None, None

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    required_cols = ['Student_ID', 'Student_Age', 'Sex', 'High_School_Type', 'Scholarship', 'Grade']
    if not all(col in df.columns for col in required_cols):
        st.error(f"–§–∞–π–ª –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å –∫–æ–ª–æ–Ω–∫–∏: {', '.join(required_cols)}")
        return None, None, None, None

    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    df['Grade_100'] = (df['Grade'] / MAX_ORIGINAL_GRADE) * MAX_TARGET_GRADE
    
    # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ 'Scholarship'
    df['Scholarship'] = df['Scholarship'].astype(str).str.replace('%', '', regex=False).astype(float)

    X = df.drop(['Student_ID', 'Grade', 'Grade_100'], axis=1)
    y = df['Grade_100']
    
    return df, X, y, df['Student_ID'] # –í–æ–∑–≤—Ä–∞—â–∞–µ–º df –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è

def get_preprocessor():
    """–°–æ–∑–¥–∞–Ω–∏–µ ColumnTransformer."""
    numerical_features = ['Student_Age', 'Weekly_Study_Hours', 'Scholarship']
    categorical_features = [
        'Sex', 'High_School_Type', 'Transportation', 'Attendance',
        'A6itional_Work', 'Sports_activity', 'Reading', 'Notes',
        'Listening_in_Class', 'Project_work'
    ]

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', StandardScaler(), numerical_features),
            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features),
        ],
        remainder='drop'
    )
    return preprocessor

@st.cache_resource
def train_model(X_train, y_train, n_iter, max_depth, learning_rate):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º RandomizedSearchCV."""
    preprocessor = get_preprocessor()
    
    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('regressor', XGBRegressor(random_state=RANDOM_STATE, objective='reg:squarederror'))
    ])

    # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è —Å–µ—Ç–∫–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–µ–ø–ª–æ—è
    param_distributions = {
        'regressor__n_estimators': [100, 200, 300],
        'regressor__learning_rate': [0.01, learning_rate],
        'regressor__max_depth': [3, max_depth, 7],
    }

    random_search = RandomizedSearchCV(
        pipeline,
        param_distributions,
        n_iter=n_iter,
        scoring='neg_mean_squared_error',
        cv=3, # –£–º–µ–Ω—å—à–∞–µ–º CV –¥–æ 3 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        verbose=0,
        n_jobs=-1,
        random_state=RANDOM_STATE
    )

    random_search.fit(X_train, y_train)
    return random_search.best_estimator_, random_search.best_params_

# --- 2. –ò–ù–¢–ï–†–§–ï–ô–° STREAMLIT ---

st.set_page_config(layout="wide", page_title="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ (XGBoost)")

st.title("üéì –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞ –Ω–∞ 100-–±–∞–ª–ª—å–Ω–æ–π —à–∫–∞–ª–µ")

# --- –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ ---
st.sidebar.header("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏")
uploaded_file = st.sidebar.file_uploader(
    "–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV-—Ñ–∞–π–ª (student_prediction.csv):", 
    type=["csv"]
)

st.sidebar.markdown("### –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã XGBoost")
n_iter = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π –ø–æ–∏—Å–∫–∞ (n_iter)", 10, 100, 30, step=10)
max_depth = st.sidebar.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –¥–µ—Ä–µ–≤–∞ (max_depth)", 3, 15, 5)
learning_rate = st.sidebar.slider("–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (learning_rate)", 0.01, 0.2, 0.1, 0.01)
test_size = st.sidebar.slider("–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (%)", 10, 50, 20) / 100


if uploaded_file is not None:
    df, X, y, student_ids = load_and_preprocess_data(uploaded_file)
    
    if df is not None:
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=RANDOM_STATE
        )
        
        # --- –û—Å–Ω–æ–≤–Ω–æ–µ –ø–æ–ª–µ ---
        
        st.header("1. –û–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö")
        st.write(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} –∫–æ–ª–æ–Ω–æ–∫.")
        st.dataframe(df.head())
        
        if st.button('üöÄ –û–±—É—á–∏—Ç—å –∏ –æ—Ü–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å'):
            with st.spinner('–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø–æ–∏—Å–∫ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤...'):
                best_model, best_params = train_model(X_train, y_train, n_iter, max_depth, learning_rate)
            
            st.success("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω—ã!")
            
            # --- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è ---
            st.header("2. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏")
            
            st.subheader("2.1. –õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
            st.json(best_params)

            # –û—Ü–µ–Ω–∫–∞
            y_pred = best_model.predict(X_test)
            mse = mean_squared_error(y_test, y_pred)
            rmse = np.sqrt(mse)
            r2 = r2_score(y_test, y_pred)
            
            col1, col2, col3 = st.columns(3)
            col1.metric("MSE", f"{mse:.4f}")
            col2.metric("RMSE (–û—à–∏–±–∫–∞)", f"{rmse:.4f} –±–∞–ª–ª–∞")
            col3.metric("R-–∫–≤–∞–¥—Ä–∞—Ç", f"{r2:.4f}")
            
            if r2 > 0.7:
                st.info(f"üéâ –ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –æ—Ç–ª–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: R-–∫–≤–∞–¥—Ä–∞—Ç > 0.7, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ {r2*100:.2f}% –¥–∏—Å–ø–µ—Ä—Å–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–æ.")

            # --- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ---
            st.subheader("2.2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
            y_pred_rounded = np.round(y_pred, 2)
            y_test_rounded = np.round(y_test, 2)
            
            test_indices = X_test.index
            original_test_ids = df.loc[test_indices, 'Student_ID']

            results_df = pd.DataFrame({
                'Student_ID': original_test_ids,
                '–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ (100)': y_test_rounded,
                '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (100)': y_pred_rounded,
                '–ê–±—Å. –û—à–∏–±–∫–∞': np.abs(y_test_rounded - y_pred_rounded)
            }).reset_index(drop=True)
            
            tab1, tab2 = st.tabs(["–õ—É—á—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è", "–•—É–¥—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"])
            with tab1:
                st.dataframe(results_df.sort_values(by='–ê–±—Å. –û—à–∏–±–∫–∞', ascending=True).head(10))
            with tab2:
                st.dataframe(results_df.sort_values(by='–ê–±—Å. –û—à–∏–±–∫–∞', ascending=False).head(10))

            # --- –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ---
            st.header("3. –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è: –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
            # –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_names_out = best_model['preprocessor'].get_feature_names_out()
            feature_names_model = [name.split('__')[-1] for name in feature_names_out]
            feature_importances = best_model['regressor'].feature_importances_
            importance_series = pd.Series(feature_importances, index=feature_names_model)
            importance_series = importance_series.sort_values(ascending=False).head(10)

            fig, ax = plt.subplots(figsize=(10, 5))
            sns.barplot(x=importance_series.values, y=importance_series.index, palette="viridis", ax=ax)
            ax.set_title('–¢–æ–ø-10 –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
            ax.set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞ (Gain)')
            ax.set_ylabel('–ü—Ä–∏–∑–Ω–∞–∫')
            st.pyplot(fig)
            
            st.markdown("---")
            st.subheader("–°–≤–æ–¥–∫–∞ –¢–æ–ø-5")
            st.dataframe(importance_series.head(5))

else:
    st.info("‚¨ÜÔ∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª `student_prediction.csv` –Ω–∞ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.")
    st.markdown("### –ü—Ä–∏–º–µ—Ä –æ–∂–∏–¥–∞–µ–º–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞–Ω–Ω—ã—Ö:")
    st.dataframe({
        'Student_ID': ['STUDENT1', 'STUDENT2'],
        'Student_Age': [20, 18],
        'Sex': ['Male', 'Female'],
        # ... –∏ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –≤–∫–ª—é—á–∞—è 'Grade'
        'Grade': [9.0, 8.5]
    })
